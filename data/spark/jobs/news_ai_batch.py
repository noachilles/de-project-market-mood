# /app/jobs/news_ai_batch.py
import os
import json
import time
from typing import Iterator, List, Dict, Any, Tuple

import requests
from pyspark.sql import SparkSession, Row
from pyspark.sql.functions import col, from_utc_timestamp, to_timestamp

# .env ë¡œë“œ
load_dotenv()

# --- í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ---
# Worker ë…¸ë“œì—ì„œë„ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ë‹¤ì‹œ ë¡œë“œí•  ì¤€ë¹„
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://gms.ssafy.io/gmsapi/api.openai.com/v1")

DB_HOST = os.getenv("POSTGRES_HOST", "postgres")
DB_NAME = os.getenv("POSTGRES_DB", "postgres")
DB_USER = os.getenv("POSTGRES_USER", "ssafyuser")
DB_PWD = os.getenv("POSTGRES_PASSWORD", "ssafy")

SUMMARY_MODEL = os.getenv("SUMMARY_MODEL", "gpt-4o-mini")
BATCH_SIZE = int(os.getenv("AI_BATCH_SIZE", "16"))
MAX_RETRIES = int(os.getenv("AI_MAX_RETRIES", "5"))


def _headers() -> Dict[str, str]:
    # Worker ë‚´ë¶€ ìœ ì‹¤ ë°©ì§€ë¥¼ ìœ„í•´ os.environì—ì„œ ì§ì ‘ ì¡°íšŒ
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        from dotenv import load_dotenv
        load_dotenv("/app/.env")
        api_key = os.environ.get("OPENAI_API_KEY")
    return {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }


def _retry_post(url: str, payload: Dict[str, Any]) -> Dict[str, Any]:
    last_err = None
    for i in range(MAX_RETRIES):
        try:
            r = requests.post(url, headers=_headers(), json=payload, timeout=60)
            if r.status_code >= 200 and r.status_code < 300:
                return r.json()
            time.sleep(min(2 ** i, 10))
        except Exception as e:
            last_err = e
            time.sleep(min(2 ** i, 10))
    raise RuntimeError(f"OpenAI request failed: {last_err}")

# --- AI ë° ì €ì¥ ë¡œì§ ---
def openai_summarize_stock_total(ticker: str, combined_content: str) -> Tuple[str, float]:
    system_instruction = (
        f"You are a senior investment strategist. Analyze news for ticker: {ticker}. "
        "Summarize the overall flow into EXACTLY 2 Korean sentences. "
        "Return JSON: { 'summary': '...', 'sentiment_score': float }"
    )
    payload = {
        "model": SUMMARY_MODEL,
        "messages": [{"role": "system", "content": system_instruction}, {"role": "user", "content": combined_content}],
        "response_format": {"type": "json_object"},
        "temperature": 0.3,
    }
    try:
        data = _retry_post(f"{OPENAI_BASE_URL}/chat/completions", payload)
        obj = json.loads(data['choices'][0]['message']['content'])
        return obj.get("summary", ""), float(obj.get("sentiment_score", 0.0))
    except Exception as e:
        print(f"âŒ AI Error for {ticker}: {e}")
        return "", 0.0

def save_stock_daily_report(ticker: str, summary: str, score: float, target_date: str) -> None:
    if not summary: return
    conn = None
    try:
        conn = psycopg2.connect(host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PWD)
        cur = conn.cursor()
        # CURRENT_DATE ëŒ€ì‹  target_dateë¥¼ ì§ì ‘ ì „ë‹¬í•˜ì—¬ ì •í™•í•œ ë‚ ì§œì— ì €ì¥
        cur.execute("""
            INSERT INTO stocks_stockdailyreport (stock_id, ai_summary, sentiment_avg, target_date)
            VALUES (%s, %s, %s, %s)
            ON CONFLICT (stock_id, target_date) 
            DO UPDATE SET ai_summary = EXCLUDED.ai_summary, sentiment_avg = EXCLUDED.sentiment_avg;
        """, (ticker, summary, score, target_date))
        conn.commit()
        print(f"âœ… Saved report for {ticker} on {target_date}")
    except Exception as e:
        print(f"âŒ DB Error for {ticker}: {e}")
    finally:
        if conn: conn.close()

# --- Partition ì²˜ë¦¬ ---
def _flush_batch(payload_rows: List[Row], target_date: str) -> int:
    SAMSUNG_CODE = "005930"  # í…ŒìŠ¤íŠ¸ìš© íƒ€ê²Ÿ ì½”ë“œ
    samsung_news_contents = []
    rows_as_dict = [r.asDict() for r in payload_rows]

    for r in rows_as_dict:
        t = r.get("title", "")
        c = (r.get("content") or r.get("body") or "")[:500]
        codes = r.get("related_stocks") or r.get("stock_codes") or []
        
        # ì‚¼ì„±ì „ì ì½”ë“œê°€ í¬í•¨ëœ ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘
        if SAMSUNG_CODE in codes:
            samsung_news_contents.append(f"ì œëª©: {t}\në‚´ìš©: {c}")

    if not samsung_news_contents:
        return 0

    # 1. ì‚¼ì„±ì „ì ê´€ë ¨ ë‰´ìŠ¤ë“¤ì„ í•˜ë‚˜ë¡œ í•©ì¹¨ (ìµœëŒ€ 10ê°œ)
    combined_text = "\n---\n".join(samsung_news_contents[:10])
    
    # 2. AI ìš”ì•½ ë° ê°ì„± ë¶„ì„ ìˆ˜í–‰
    print(f"ğŸ¤– ì‚¼ì„±ì „ì ê´€ë ¨ ë‰´ìŠ¤ {len(samsung_news_contents)}ê±´ ë¶„ì„ ì¤‘...")
    summary, score = openai_summarize_stock_total(SAMSUNG_CODE, combined_text)
    
    # 3. DB ì €ì¥
    if summary:
        save_stock_daily_report(SAMSUNG_CODE, summary, score, target_date)
        print(f"âœ¨ [SAMSUNG] {target_date} ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ!")
        return 1
    
    return 0

def process_partition(rows: Iterator[Row], target_date: str) -> Iterator[int]:
    # ì‚¼ì„±ì „ìëŠ” ë°ì´í„° ì–‘ì´ ì ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ íŒŒí‹°ì…˜ ì „ì²´ë¥¼ ëª¨ì•„ì„œ í•œ ë²ˆì— ì²˜ë¦¬
    batch = list(rows)
    if batch:
        yield _flush_batch(batch, target_date)
    else:
        yield 0

def main():
    spark = SparkSession.builder.appName("news-ai-batch-samsung").getOrCreate()
    target_date = sys.argv[1] if len(sys.argv) > 1 else datetime.now().strftime("%Y-%m-%d")
    
    path = f"/opt/data-lake/news_enriched/dt={target_date}"
    if not os.path.exists(path):
        print(f"âŒ '{target_date}' ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    try:
        df = spark.read.parquet(path)
        
        # RDDë¡œ ë³€í™˜í•˜ì—¬ ì‚¼ì„±ì „ì ì „ìš© ë¡œì§ ìˆ˜í–‰
        final_counts = df.rdd.mapPartitions(lambda rows: process_partition(rows, target_date)).collect()
        
        total_reports = sum(final_counts)
        if total_reports > 0:
            print(f"âœ… [DONE] ì‚¼ì„±ì „ì ë¦¬í¬íŠ¸ ìƒì„± ì„±ê³µ!")
        else:
            print(f"âš ï¸ í•´ë‹¹ ë‚ ì§œ({target_date}) ë‰´ìŠ¤ ì¤‘ ì‚¼ì„±ì „ì ê´€ë ¨ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
            
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
