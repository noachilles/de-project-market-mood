# /app/jobs/news_ai_batch.py
import os
import json
import time
from typing import Iterator, List, Dict, Any, Tuple

import requests
from pyspark.sql import SparkSession, Row
<<<<<<< HEAD
from pyspark.sql.functions import col, from_utc_timestamp, to_timestamp

# .env ë¡œë“œ
load_dotenv()

# --- í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ---
# Worker ë…¸ë“œì—ì„œë„ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ë‹¤ì‹œ ë¡œë“œí•  ì¤€ë¹„
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://gms.ssafy.io/gmsapi/api.openai.com/v1")

DB_HOST = os.getenv("POSTGRES_HOST", "postgres")
DB_NAME = os.getenv("POSTGRES_DB", "postgres")
DB_USER = os.getenv("POSTGRES_USER", "ssafyuser")
DB_PWD = os.getenv("POSTGRES_PASSWORD", "ssafy")

SUMMARY_MODEL = os.getenv("SUMMARY_MODEL", "gpt-4o-mini")
BATCH_SIZE = int(os.getenv("AI_BATCH_SIZE", "16"))
=======
from pyspark.sql.functions import col, concat_ws, coalesce, lit

OPENAI_BASE_URL = "https://api.openai.com/v1"
ES_BASE_URL = os.getenv("ES_BASE_URL", "http://elasticsearch:9200")
ES_INDEX = os.getenv("ES_INDEX", "news-ai-vector")

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
EMBED_MODEL = os.getenv("EMBED_MODEL", "text-embedding-3-large")
EMBED_DIMS = int(os.getenv("EMBED_DIMS", "1536"))  # ES dimsì— ë§ì¶° 1536 ê¶Œì¥
SUMMARY_MODEL = os.getenv("SUMMARY_MODEL", "gpt-4.1-mini")  # ê°€ë²¼ìš´ ëª¨ë¸ë¡œ ì‹œì‘ ì¶”ì²œ

# ë¹„ìš©/ì†ë„ìš© íŠœë‹
BATCH_SIZE = int(os.getenv("AI_BATCH_SIZE", "16"))      # OpenAI API í˜¸ì¶œ ë°°ì¹˜
SLEEP_BETWEEN_CALLS = float(os.getenv("AI_SLEEP", "0.0"))
>>>>>>> parent of 79a31d5 (Merge branch 'MM-32' of https://lab.ssafy.com/dtmg1ejk/de-project into MM-32)
MAX_RETRIES = int(os.getenv("AI_MAX_RETRIES", "5"))


def _headers() -> Dict[str, str]:
<<<<<<< HEAD
    # Worker ë‚´ë¶€ ìœ ì‹¤ ë°©ì§€ë¥¼ ìœ„í•´ os.environì—ì„œ ì§ì ‘ ì¡°íšŒ
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        from dotenv import load_dotenv
        load_dotenv("/app/.env")
        api_key = os.environ.get("OPENAI_API_KEY")
=======
    if not OPENAI_API_KEY:
        raise RuntimeError("OPENAI_API_KEY is empty. Set it in environment.")
>>>>>>> parent of 79a31d5 (Merge branch 'MM-32' of https://lab.ssafy.com/dtmg1ejk/de-project into MM-32)
    return {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }


def _retry_post(url: str, payload: Dict[str, Any]) -> Dict[str, Any]:
    last_err = None
    for i in range(MAX_RETRIES):
        try:
            r = requests.post(url, headers=_headers(), json=payload, timeout=60)
            if r.status_code >= 200 and r.status_code < 300:
                return r.json()
<<<<<<< HEAD
            time.sleep(min(2 ** i, 10))
        except Exception as e:
            last_err = e
            time.sleep(min(2 ** i, 10))
    raise RuntimeError(f"OpenAI request failed: {last_err}")

# --- AI ë° ì €ì¥ ë¡œì§ ---
def openai_summarize_stock_total(ticker: str, combined_content: str) -> Tuple[str, float]:
    system_instruction = (
        f"You are a senior investment strategist. Analyze news for ticker: {ticker}. "
        "Summarize the overall flow into EXACTLY 2 Korean sentences. "
        "Return JSON: { 'summary': '...', 'sentiment_score': float }"
    )
    payload = {
        "model": SUMMARY_MODEL,
        "messages": [{"role": "system", "content": system_instruction}, {"role": "user", "content": combined_content}],
        "response_format": {"type": "json_object"},
        "temperature": 0.3,
    }
    try:
        data = _retry_post(f"{OPENAI_BASE_URL}/chat/completions", payload)
        obj = json.loads(data['choices'][0]['message']['content'])
        return obj.get("summary", ""), float(obj.get("sentiment_score", 0.0))
    except Exception as e:
        print(f"âŒ AI Error for {ticker}: {e}")
        return "", 0.0

def save_stock_daily_report(ticker: str, summary: str, score: float, target_date: str) -> None:
    if not summary: return
    conn = None
    try:
        conn = psycopg2.connect(host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PWD)
        cur = conn.cursor()
        # CURRENT_DATE ëŒ€ì‹  target_dateë¥¼ ì§ì ‘ ì „ë‹¬í•˜ì—¬ ì •í™•í•œ ë‚ ì§œì— ì €ì¥
        cur.execute("""
            INSERT INTO stocks_stockdailyreport (stock_id, ai_summary, sentiment_avg, target_date)
            VALUES (%s, %s, %s, %s)
            ON CONFLICT (stock_id, target_date) 
            DO UPDATE SET ai_summary = EXCLUDED.ai_summary, sentiment_avg = EXCLUDED.sentiment_avg;
        """, (ticker, summary, score, target_date))
        conn.commit()
        print(f"âœ… Saved report for {ticker} on {target_date}")
    except Exception as e:
        print(f"âŒ DB Error for {ticker}: {e}")
    finally:
        if conn: conn.close()

# --- Partition ì²˜ë¦¬ ---
def _flush_batch(payload_rows: List[Row], target_date: str) -> int:
    SAMSUNG_CODE = "005930"  # í…ŒìŠ¤íŠ¸ìš© íƒ€ê²Ÿ ì½”ë“œ
    samsung_news_contents = []
    rows_as_dict = [r.asDict() for r in payload_rows]

    for r in rows_as_dict:
        t = r.get("title", "")
        c = (r.get("content") or r.get("body") or "")[:500]
        codes = r.get("related_stocks") or r.get("stock_codes") or []
        
        # ì‚¼ì„±ì „ì ì½”ë“œê°€ í¬í•¨ëœ ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘
        if SAMSUNG_CODE in codes:
            samsung_news_contents.append(f"ì œëª©: {t}\në‚´ìš©: {c}")

    if not samsung_news_contents:
        return 0

    # 1. ì‚¼ì„±ì „ì ê´€ë ¨ ë‰´ìŠ¤ë“¤ì„ í•˜ë‚˜ë¡œ í•©ì¹¨ (ìµœëŒ€ 10ê°œ)
    combined_text = "\n---\n".join(samsung_news_contents[:10])
    
    # 2. AI ìš”ì•½ ë° ê°ì„± ë¶„ì„ ìˆ˜í–‰
    print(f"ğŸ¤– ì‚¼ì„±ì „ì ê´€ë ¨ ë‰´ìŠ¤ {len(samsung_news_contents)}ê±´ ë¶„ì„ ì¤‘...")
    summary, score = openai_summarize_stock_total(SAMSUNG_CODE, combined_text)
    
    # 3. DB ì €ì¥
    if summary:
        save_stock_daily_report(SAMSUNG_CODE, summary, score, target_date)
        print(f"âœ¨ [SAMSUNG] {target_date} ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ!")
        return 1
    
    return 0

def process_partition(rows: Iterator[Row], target_date: str) -> Iterator[int]:
    # ì‚¼ì„±ì „ìëŠ” ë°ì´í„° ì–‘ì´ ì ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ íŒŒí‹°ì…˜ ì „ì²´ë¥¼ ëª¨ì•„ì„œ í•œ ë²ˆì— ì²˜ë¦¬
    batch = list(rows)
    if batch:
        yield _flush_batch(batch, target_date)
    else:
        yield 0

def main():
    spark = SparkSession.builder.appName("news-ai-batch-samsung").getOrCreate()
    target_date = sys.argv[1] if len(sys.argv) > 1 else datetime.now().strftime("%Y-%m-%d")
    
    path = f"/opt/data-lake/news_enriched/dt={target_date}"
    if not os.path.exists(path):
        print(f"âŒ '{target_date}' ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    try:
        df = spark.read.parquet(path)
        
        # RDDë¡œ ë³€í™˜í•˜ì—¬ ì‚¼ì„±ì „ì ì „ìš© ë¡œì§ ìˆ˜í–‰
        final_counts = df.rdd.mapPartitions(lambda rows: process_partition(rows, target_date)).collect()
        
        total_reports = sum(final_counts)
        if total_reports > 0:
            print(f"âœ… [DONE] ì‚¼ì„±ì „ì ë¦¬í¬íŠ¸ ìƒì„± ì„±ê³µ!")
        else:
            print(f"âš ï¸ í•´ë‹¹ ë‚ ì§œ({target_date}) ë‰´ìŠ¤ ì¤‘ ì‚¼ì„±ì „ì ê´€ë ¨ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
            
    finally:
        spark.stop()
=======
            # 429/5xx ì¬ì‹œë„
            if r.status_code in (429, 500, 502, 503, 504):
                time.sleep(min(2 ** i, 20))
                continue
            # ê·¸ ì™¸ëŠ” ë°”ë¡œ ì—ëŸ¬
            raise RuntimeError(f"HTTP {r.status_code}: {r.text[:500]}")
        except Exception as e:
            last_err = e
            time.sleep(min(2 ** i, 20))
    raise RuntimeError(f"OpenAI request failed after retries: {last_err}")


def openai_embed(texts: List[str]) -> List[List[float]]:
    """
    /v1/embeddings
    - model: text-embedding-3-large
    - dimensions: 1536 (ES ë§¤í•‘ê³¼ ë§ì¶”ê¸°)
    """
    payload = {
        "model": EMBED_MODEL,
        "input": texts,
        "dimensions": EMBED_DIMS,
    }
    data = _retry_post(f"{OPENAI_BASE_URL}/embeddings", payload)
    # data["data"] ëŠ” input ìˆœì„œëŒ€ë¡œ list
    return [item["embedding"] for item in data["data"]]


def openai_summarize_and_sentiment(title: str, content: str) -> Tuple[str, float]:
    """
    /v1/responses ë¡œ "ìš”ì•½ + ê°ì •ì ìˆ˜(-1~1)"ë¥¼ JSONìœ¼ë¡œ ë°›ê¸°
    """
    instruction = (
        "You are a finance news analyst. "
        "Return JSON only with keys: summary (Korean, <= 2 sentences), sentiment_score (float -1 to 1)."
    )
    user_input = f"[TITLE]\n{title}\n\n[CONTENT]\n{content}"

    payload = {
        "model": SUMMARY_MODEL,
        "instructions": instruction,
        "input": user_input,
        "text": {"format": {"type": "text"}},
        "temperature": 0.2,
        "max_output_tokens": 200,
    }
    data = _retry_post(f"{OPENAI_BASE_URL}/responses", payload)

    # ë¬¸ì„œ ì˜ˆì‹œì²˜ëŸ¼ output -> message -> content -> output_text í…ìŠ¤íŠ¸ë¥¼ ë½‘ëŠ”ë‹¤
    # :contentReference[oaicite:3]{index=3}
    out_text = ""
    for item in data.get("output", []):
        if item.get("type") == "message":
            for c in item.get("content", []):
                if c.get("type") == "output_text":
                    out_text += c.get("text", "")
    out_text = out_text.strip()

    # JSON only ê°•ì œí–ˆìœ¼ë‹ˆ íŒŒì‹±
    try:
        obj = json.loads(out_text)
        summary = str(obj.get("summary", "")).strip()
        score = float(obj.get("sentiment_score", 0.0))
        # clamp
        score = max(-1.0, min(1.0, score))
        return summary, score
    except Exception:
        # ì‹¤íŒ¨ ì‹œ fallback
        return out_text[:300], 0.0


def es_bulk_upsert(docs: List[Dict[str, Any]]) -> None:
    """
    Elasticsearch _bulk update(doc_as_upsert)
    """
    if not docs:
        return

    lines = []
    for d in docs:
        _id = d["news_id"]
        meta = {"update": {"_index": ES_INDEX, "_id": _id}}
        body = {"doc": d, "doc_as_upsert": True}
        lines.append(json.dumps(meta, ensure_ascii=False))
        lines.append(json.dumps(body, ensure_ascii=False))

    bulk_body = "\n".join(lines) + "\n"
    r = requests.post(
        f"{ES_BASE_URL}/_bulk",
        headers={"Content-Type": "application/x-ndjson"},
        data=bulk_body.encode("utf-8"),
        timeout=60,
    )
    if r.status_code < 200 or r.status_code >= 300:
        raise RuntimeError(f"ES bulk failed {r.status_code}: {r.text[:500]}")
    resp = r.json()
    if resp.get("errors"):
        # ì–´ë–¤ ë¬¸ì„œê°€ ì‹¤íŒ¨í–ˆëŠ”ì§€ ìµœì†Œí•œë§Œ ì¶œë ¥
        items = resp.get("items", [])
        bad = [it for it in items if list(it.values())[0].get("error")]
        raise RuntimeError(f"ES bulk had errors. sample={bad[:2]}")


def process_partition(rows: Iterator[Row]) -> Iterator[int]:
    """
    Spark executorì—ì„œ partition ë‹¨ìœ„ ì²˜ë¦¬:
    - rows -> batch
    - summary/sentiment + embedding
    - ES upsert
    """
    batch = []
    count = 0

    for r in rows:
        batch.append(r)
        if len(batch) >= BATCH_SIZE:
            count += _flush_batch(batch)
            batch = []

    if batch:
        count += _flush_batch(batch)

    yield count


def _flush_batch(batch: List[Row]) -> int:
    texts = []
    payload_rows = []

    for r in batch:
        title = r["title"] or ""
        content = r["content"] or ""
        combined = f"{title}\n\n{content}".strip()
        texts.append(combined)
        payload_rows.append(r)

    # 1) ìš”ì•½/ê°ì •
    summaries = []
    sentiments = []
    for r in payload_rows:
        s, sc = openai_summarize_and_sentiment(r["title"] or "", r["content"] or "")
        summaries.append(s)
        sentiments.append(sc)
        if SLEEP_BETWEEN_CALLS:
            time.sleep(SLEEP_BETWEEN_CALLS)

    # 2) ì„ë² ë”© (ë°°ì¹˜ í˜¸ì¶œ)
    vectors = openai_embed(texts)
    if SLEEP_BETWEEN_CALLS:
        time.sleep(SLEEP_BETWEEN_CALLS)

    # 3) ES ë¬¸ì„œ êµ¬ì„± + upsert
    docs = []
    for r, summary, score, vec in zip(payload_rows, summaries, sentiments, vectors):
        docs.append({
            "news_id": r["news_id"],
            "title": r["title"],
            "content_summary": summary,
            "stock_codes": r["stock_codes"] or [],
            "published_at": r["published_at"],  # ë¬¸ìì—´/íƒ€ì„ìŠ¤íƒ¬í”„ ëª¨ë‘ ê°€ëŠ¥(ES date íŒŒì„œê°€ ì²˜ë¦¬)
            "sentiment_score": score,
            "embedding": vec,
        })

    es_bulk_upsert(docs)
    return len(docs)


def main():
    input_path = os.getenv("INPUT_PATH", "/app/lake/news_raw/*.parquet")

    spark = (
        SparkSession.builder
        .appName("news-ai-batch")
        .getOrCreate()
    )

    # Parquet ìŠ¤í‚¤ë§ˆ ì˜ˆì‹œ:
    # news_id, title, content, stock_codes(array<string>), published_at
    df = spark.read.parquet(input_path).select(
        col("news_id"),
        col("title"),
        col("content"),
        col("stock_codes"),
        col("published_at"),
    ).where(col("news_id").isNotNull())

    # executorì—ì„œ requests/OpenAIë¥¼ ì“°ë¯€ë¡œ RDDë¡œ ë‚´ë¦¬ê³  partition ì²˜ë¦¬
    counts = df.rdd.mapPartitions(process_partition).collect()
    total = sum(counts)
    print(f"[DONE] processed={total} input_path={input_path} index={ES_INDEX}")

>>>>>>> parent of 79a31d5 (Merge branch 'MM-32' of https://lab.ssafy.com/dtmg1ejk/de-project into MM-32)

if __name__ == "__main__":
    main()
