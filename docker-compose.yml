version: '3.8'

# [ì´ ì»¨í…Œì´ë„ˆ ê°œìˆ˜: 15ê°œ]

services:
  # =========================================
  # 1. Message Queue (ë°ì´í„°ì˜ í˜ˆê´€)
  # =========================================
  
  # [1] Zookeeper: Kafka ë¸Œë¡œì»¤ë“¤ì„ ê´€ë¦¬í•˜ê³  ì¡°ì •í•˜ëŠ” ì½”ë””ë„¤ì´í„°
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.1
    container_name: zookeeper
    ports:
      - 2181:2181
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  # [2] Kafka: ì‹¤ì‹œê°„ ì£¼ì‹/ë‰´ìŠ¤ ë°ì´í„°ê°€ ì§€ë‚˜ê°€ëŠ” ë©”ì¸ ë©”ì‹œì§€ í
  kafka:
    image: confluentinc/cp-kafka:7.6.1
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - 9094:9094
      - 9092:9092
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:9092,EXTERNAL://localhost:9094
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:9094
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL

      # ë‹¨ì¼ ë¸Œë¡œì»¤ ì‹¤í–‰ì„ ìœ„í•œ í•„ìˆ˜ ì„¤ì • (ë³µì œ ê³„ìˆ˜ 1)
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1

    healthcheck:
      test: nc -z localhost 9092 || exit 1  # 9092 í¬íŠ¸ê°€ ì—´ë ¸ëŠ”ì§€ í™•ì¸
      interval: 10s
      timeout: 5s
      retries: 5

  # [3] Kafka UI: í† í”½ì˜ ë°ì´í„° íë¦„ê³¼ ìƒíƒœë¥¼ ì›¹ì—ì„œ í™•ì¸í•˜ëŠ” ë„êµ¬
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka
    ports:
      - 8080:8080
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAP_SERVERS: kafka:9092

  # =========================================
  # 2. Data Processing (ë°ì´í„° ê°€ê³µ ê³µì¥)
  # =========================================
  
  # [4] Flink JobManager: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì‘ì—…ì„ ê´€ë¦¬í•˜ëŠ” ë§ˆìŠ¤í„° ë…¸ë“œ
  flink-jobmanager:
    build: ./data/flink
    container_name: flink-jobmanager
    ports:
      - "8081:8081"
    command: jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        rest.bind-address: 0.0.0.0
    volumes:
      # ì½”ë“œëŠ” jobs í´ë”ì— ë”°ë¡œ ë‹´ì•„ì„œ ì»¨í…Œì´ë„ˆì˜ /opt/flink/jobsì— ì—°ê²°
      - ./data/flink/jobs:/opt/flink/jobs
      - ./data/volumes/data-lake:/opt/data-lake
    depends_on:
      - kafka

  # [5] Flink TaskManager: ì‹¤ì œë¡œ ë°ì´í„°ë¥¼ ì‹¤ì‹œê°„ ì²˜ë¦¬/ì €ì¥í•˜ëŠ” ì›Œì»¤ ë…¸ë“œ
  flink-taskmanager:
    build: ./data/flink
    container_name: flink-taskmanager
    command: taskmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 2
    volumes:
      - ./data/flink/jobs:/opt/flink/jobs
      - ./data/volumes/data-lake:/opt/data-lake
    depends_on:
      - flink-jobmanager

  flink-job-submitter:
    build: ./data/flink  # JobManagerì™€ ê°™ì€ ì´ë¯¸ì§€(í™˜ê²½) ì‚¬ìš©
    container_name: flink-job-submitter
    depends_on:
      - flink-jobmanager
      - kafka
    volumes:
      - ./data/flink/jobs:/opt/flink/jobs
    # [ìë™í™” ë¡œì§]
    # 1. JobManagerê°€ ì™„ì „íˆ ì¼œì§ˆ ë•Œê¹Œì§€ 10ì´ˆ ì •ë„ ê¸°ë‹¤ë¦¼ (ì•ˆì „ì¥ì¹˜)
    # 2. flink run ëª…ë ¹ì–´ë¡œ Job ì œì¶œ (-d: Detached ëª¨ë“œë¡œ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰)
    command: >
      bash -c "
      echo 'Waiting for Flink JobManager...' &&
      sleep 20 &&
      echo 'Submitting Flink Job...' &&
      /opt/flink/bin/flink run -m flink-jobmanager:8081 -py /opt/flink/jobs/news_processing_job.py -d
      "

  # [6] Spark Master: ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ë¶„ì„ ì‘ì—…ì„ ê´€ë¦¬í•˜ëŠ” ë§ˆìŠ¤í„° ë…¸ë“œ
  spark-master:
    build: 
      context: ./data/spark 
    container_name: spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "9090:8080"
      - "7077:7077"
    volumes:
      - ./data/spark:/app
      - ./data/volumes/data-lake:/opt/data-lake
    environment:
      - SPARK_NO_DAEMONIZE=true

  # [7] Spark Worker: ë§ˆìŠ¤í„°ì˜ ì§€ì‹œëŒ€ë¡œ ê³¼ê±° ë°ì´í„°ë¥¼ ë¶„ì„í•˜ëŠ” ì›Œì»¤ ë…¸ë“œ
  spark-worker:
    build: 
      context: ./data/spark 
    container_name: spark-worker
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_NO_DAEMONIZE=true
    volumes:
      - ./data/spark:/app
      - ./data/volumes/data-lake:/opt/data-lake
    depends_on:
      - spark-master

  # =========================================
  # 3. Databases (ë°ì´í„° ì €ì¥ì†Œ)
  # =========================================
  
  # [8] Postgres (TimescaleDB): ì‚¬ìš©ì ì •ë³´ ë° ì£¼ê°€ ì°¨íŠ¸ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” RDB
  postgres:
    image: timescale/timescaledb:latest-pg15
    container_name: postgres
    restart: always
    volumes:
      - ./data/volumes/postgres-data:/var/lib/postgresql/data
      - ./data/config/postgres:/docker-entrypoint-initdb.d
    ports:
      - "5433:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}

  # [9] Redis: í˜„ì¬ê°€ ë“± ì´ˆë‹¨ê¸° ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ì œê³µí•˜ëŠ” ìºì‹œ ì €ì¥ì†Œ
  redis:
    image: redis:alpine
    container_name: redis
    ports:
      - "6379:6379"

  # [10] Elasticsearch: ë‰´ìŠ¤ ê¸°ì‚¬ì™€ ë²¡í„°ë¥¼ ì €ì¥í•˜ì—¬ ê²€ìƒ‰/AIë¥¼ ì§€ì›í•˜ëŠ” ê²€ìƒ‰ì—”ì§„
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.10
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - ./data/volumes/es-data:/usr/share/elasticsearch/data

  # [11] Kibana: Elasticsearch ë°ì´í„°ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•˜ëŠ” ëŒ€ì‹œë³´ë“œ
  kibana:
    image: docker.elastic.co/kibana/kibana:7.17.10
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch

  # =========================================
  # 4. App Layer (ìˆ˜ì§‘ ë° ì„œë¹„ìŠ¤)
  # =========================================
  
  # [12] news-producers: RSSì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ Kafkaë¡œ ë³´ë‚´ëŠ” íŒŒì´ì¬ ìˆ˜ì§‘ê¸°
  news-producer:
    container_name: news-producer
    build:
      context: ./data/producers  # ì—¬ê¸°ì— Dockerfileê³¼ requirements.txtê°€ ìˆì–´ì•¼ í•¨
    image: my-news-producer:1.0
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
    restart: always

  # [13] Websocket Server: ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ë¸Œë¼ìš°ì €ë¡œ ì¦‰ì‹œ ë°€ì–´ì£¼ëŠ” ì¤‘ê³„ ì„œë²„
  websocket-server:
    image: python:3.9-slim
    container_name: websocket-server
    depends_on:
      - kafka
    volumes:
      - ./back-end/websocket:/app
    working_dir: /app
    ports:
      - "8765:8765"
    command: >
      bash -c "pip install websockets kafka-python && python server.py"

  # [14] Backend API: ì‚¬ìš©ì ìš”ì²­, ë¡œê·¸ì¸, DB ì¡°íšŒë¥¼ ì²˜ë¦¬í•˜ëŠ” Django ì„œë²„
  backend:
    container_name: backend
    build:
      context: ./back-end
      dockerfile: api/Dockerfile
    ports:
      - "8000:8000"
    working_dir: /app
    command: python manage.py runserver 0.0.0.0:8000
    volumes:
      - ./back-end:/app
      - ./data:/data
    depends_on:
      - kafka
      - postgres
      - redis
      - elasticsearch
    env_file:
      - .env

  # [15] Frontend: ì‚¬ìš©ìê°€ ì ‘ì†í•˜ëŠ” ì›¹ í™”ë©´(UI) ì„œë²„
  frontend:
    container_name: frontend
    build:
      context: ./front-end
    ports:
      - "80:80"
    depends_on:
      - backend

  stock-consumer:
    # âœ… backendë‘ ê°™ì€ ì´ë¯¸ì§€/ë¹Œë“œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¬ì‚¬ìš© (ë‘˜ ì¤‘ í”„ë¡œì íŠ¸ì— ë§ëŠ” ê±¸ë¡œ íƒ1)
    # 1) backendê°€ buildë¡œ ë˜ì–´ ìˆìœ¼ë©´ build ì¬ì‚¬ìš©:
    build:
      context: ./back-end
    # 2) backendê°€ imageë¡œ ë˜ì–´ ìˆìœ¼ë©´ image ì¬ì‚¬ìš©:
    # image: <backend-image-name>
      dockerfile: api/Dockerfile
    container_name: stock-consumer
    working_dir: /app
    command: ["python", "consumers/stock_consumer.py"]
    depends_on:
      - kafka
      - redis
      - backend
    environment:
      # ğŸ”» consumer ì½”ë“œê°€ ì–´ë–¤ envë¥¼ ì“°ëŠ”ì§€ ëª°ë¼ë„, ë³´í†µ ì•„ë˜ë¡œ ë§ì¶°ë‘ë©´ í¸í•¨
      KAFKA_BOOTSTRAP_SERVERS: "kafka:9092"
      REDIS_HOST: "redis"
      REDIS_PORT: "6379"
      # í•„ìš”í•˜ë©´ topic/groupë„ ì—¬ê¸°ì„œ ì§€ì •
      # KAFKA_TOPIC: "stock-price"
      # KAFKA_GROUP_ID: "stock-consumer"
    volumes:
      - ./back-end:/app
    restart: unless-stopped
  
  frontend-dev:
    image: node:20-bookworm-slim
    container_name: frontend-dev
    working_dir: /app
    volumes:
      - ./front-end:/app
    ports:
      - "5173:5173"
    environment:
      - VITE_API_BASE=http://localhost:8000
    command: sh -lc "npm install && npm run dev -- --host 0.0.0.0 --port 5173"
    depends_on:
      - backend

  stock-producer:
    container_name: stock-producer
    build:
      context: ./data/producers
      dockerfile: Dockerfile
    command: python kis_producer.py
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      # .env íŒŒì¼ì— í‚¤ê°’ì´ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•¨!
      - KIS_APP_KEY=${KIS_APP_KEY}
      - KIS_APP_SECRET=${KIS_APP_SECRET}
    env_file:
      - .env
    restart: always

  stock-bridge:
    container_name: stock-bridge
    build:
      context: ./back-end
      dockerfile: api/Dockerfile
    command: python manage.py run_stock_bridge
    environment:
      - PYTHONUNBUFFERED=1
    volumes:
      - ./back-end:/app
    depends_on:
      - redis
      - backend
    env_file:
      - .env
    restart: unless-stopped

networks:
  default:
    name: de-project-network

