version: '3.8'

# [총 컨테이너 개수: 15개]

services:
  # =========================================
  # 1. Message Queue (데이터의 혈관)
  # =========================================
  
  # [1] Zookeeper: Kafka 브로커들을 관리하고 조정하는 코디네이터
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.1
    container_name: zookeeper
    ports:
      - 2181:2181
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  # [2] Kafka: 실시간 주식/뉴스 데이터가 지나가는 메인 메시지 큐
  kafka:
    image: confluentinc/cp-kafka:7.6.1
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - 9094:9094
      - 9092:9092
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:9092,EXTERNAL://localhost:9094
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:9094
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL

      # 단일 브로커 실행을 위한 필수 설정 (복제 계수 1)
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1

    healthcheck:
      test: nc -z localhost 9092 || exit 1  # 9092 포트가 열렸는지 확인
      interval: 10s
      timeout: 5s
      retries: 5

  # [3] Kafka UI: 토픽의 데이터 흐름과 상태를 웹에서 확인하는 도구
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka
    ports:
      - 8080:8080
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAP_SERVERS: kafka:9092

  # =========================================
  # 2. Data Processing (데이터 가공 공장)
  # =========================================
  
  # [4] Flink JobManager: 실시간 스트림 처리 작업을 관리하는 마스터 노드
  flink-jobmanager:
    build: ./data/flink
    container_name: flink-jobmanager
    ports:
      - "8081:8081"
    command: jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        rest.bind-address: 0.0.0.0
    volumes:
      # 코드는 jobs 폴더에 따로 담아서 컨테이너의 /opt/flink/jobs에 연결
      - ./data/flink/jobs:/opt/flink/jobs
      - ./data/volumes/data-lake:/opt/data-lake
    depends_on:
      - kafka

  # [5] Flink TaskManager: 실제로 데이터를 실시간 처리/저장하는 워커 노드
  flink-taskmanager:
    build: ./data/flink
    container_name: flink-taskmanager
    command: taskmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 2
    volumes:
      - ./data/flink/jobs:/opt/flink/jobs
      - ./data/volumes/data-lake:/opt/data-lake
    depends_on:
      - flink-jobmanager

  flink-job-submitter:
    build: ./data/flink  # JobManager와 같은 이미지(환경) 사용
    container_name: flink-job-submitter
    depends_on:
      - flink-jobmanager
      - kafka
    volumes:
      - ./data/flink/jobs:/opt/flink/jobs
    # [자동화 로직]
    # 1. JobManager가 완전히 켜질 때까지 10초 정도 기다림 (안전장치)
    # 2. flink run 명령어로 Job 제출 (-d: Detached 모드로 백그라운드 실행)
    command: >
      bash -c "
      echo 'Waiting for Flink JobManager...' &&
      sleep 20 &&
      echo 'Submitting Flink Job...' &&
      /opt/flink/bin/flink run -m flink-jobmanager:8081 -py /opt/flink/jobs/news_processing_job.py -d
      "

  # [6] Spark Master: 대용량 배치 분석 작업을 관리하는 마스터 노드
  spark-master:
    build: 
      context: ./data/spark 
    container_name: spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "9090:8080"
      - "7077:7077"
    volumes:
      - ./data/spark:/app
      - ./data/volumes/data-lake:/opt/data-lake
    environment:
      - SPARK_NO_DAEMONIZE=true

  # [7] Spark Worker: 마스터의 지시대로 과거 데이터를 분석하는 워커 노드
  spark-worker:
    build: 
      context: ./data/spark 
    container_name: spark-worker
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_NO_DAEMONIZE=true
    volumes:
      - ./data/spark:/app
      - ./data/volumes/data-lake:/opt/data-lake
    depends_on:
      - spark-master

  # =========================================
  # 3. Databases (데이터 저장소)
  # =========================================
  
  # [8] Postgres (TimescaleDB): 사용자 정보 및 주가 차트 데이터를 저장하는 RDB
  postgres:
    image: timescale/timescaledb:latest-pg15
    container_name: postgres
    restart: always
    volumes:
      - ./data/volumes/postgres-data:/var/lib/postgresql/data
      - ./data/config/postgres:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}

  # [9] Redis: 현재가 등 초단기 데이터를 빠르게 제공하는 캐시 저장소
  redis:
    image: redis:alpine
    container_name: redis
    ports:
      - "6379:6379"

  # [10] Elasticsearch: 뉴스 기사와 벡터를 저장하여 검색/AI를 지원하는 검색엔진
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.10
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - ./data/volumes/es-data:/usr/share/elasticsearch/data

  # [11] Kibana: Elasticsearch 데이터를 시각적으로 확인하는 대시보드
  kibana:
    image: docker.elastic.co/kibana/kibana:7.17.10
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch

  # =========================================
  # 4. App Layer (수집 및 서비스)
  # =========================================
  
  # [12] news-producers: RSS에서 데이터를 수집해 Kafka로 보내는 파이썬 수집기
  news-producer:
    container_name: news-producer
    build:
      context: ./data/producers  # 여기에 Dockerfile과 requirements.txt가 있어야 함
    image: my-news-producer:1.0
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
    restart: always

  # [13] Websocket Server: 실시간 데이터를 브라우저로 즉시 밀어주는 중계 서버
  websocket-server:
    image: python:3.9-slim
    container_name: websocket-server
    depends_on:
      - kafka
    volumes:
      - ./back-end/websocket:/app
    working_dir: /app
    ports:
      - "8765:8765"
    command: >
      bash -c "pip install websockets kafka-python && python server.py"

  # [14] Backend API: 사용자 요청, 로그인, DB 조회를 처리하는 Django 서버
  backend:
    container_name: backend
    build:
      context: ./back-end
      dockerfile: api/Dockerfile
    ports:
      - "8000:8000"
    working_dir: /app
    command: python manage.py runserver 0.0.0.0:8000
    volumes:
      - ./back-end:/app
    depends_on:
      - kafka
      - postgres
      - redis
      - elasticsearch
    env_file:
      - .env

  # [15] Frontend: 사용자가 접속하는 웹 화면(UI) 서버
  frontend:
    container_name: frontend
    build:
      context: ./front-end
    ports:
      - "80:80"
    depends_on:
      - backend

  stock-consumer:
    container_name: stock-consumer
    build:
      context: ./back-end
      dockerfile: api/Dockerfile
    command: python consumers/stock_consumer.py
    restart: unless-stopped
    volumes:
      - ./back-end:/app
    depends_on:
      - kafka
      - postgres
      - redis
    env_file:
      - .env

networks:
  default:
    name: de-project-network

