import feedparser
import requests
from bs4 import BeautifulSoup
import time
import random
import json
import csv
import os
from kafka import KafkaProducer
from datetime import datetime
import dateutil.parser # pip install python-dateutil í•„ìˆ˜

# --- ì„¤ì • ---
KAFKA_BROKER = 'kafka:9092'
KAFKA_TOPIC = 'news_articles'

RSS_URLS = {
    "ê²½ì œ": "https://www.khan.co.kr/rss/rssdata/economy_news.xml",
    "êµ­ì œ": "https://www.khan.co.kr/rss/rssdata/kh_world.xml",
}

STATE_FILE = "producer_state.json"
CSV_FILE = f'khan_news_{datetime.now().strftime("%Y%m%d")}.csv'

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def create_producer():
    try:
        producer = KafkaProducer(
            bootstrap_servers=[KAFKA_BROKER],
            api_version=(0, 10, 1),
            value_serializer=lambda v: json.dumps(v, ensure_ascii=False).encode('utf-8')
        )
        print("âœ… Kafka Producer ì—°ê²° ì„±ê³µ")
        return producer
    except Exception as e:
        print(f"âŒ Kafka ì—°ê²° ì‹¤íŒ¨: {e}")
        return None

# --- [í•µì‹¬ 1] ìƒíƒœ ê´€ë¦¬ í•¨ìˆ˜ ---
def load_last_published_at():
    if os.path.exists(STATE_FILE):
        with open(STATE_FILE, 'r') as f:
            return json.load(f)
    return {}

def save_last_published_at(state):
    with open(STATE_FILE, 'w') as f:
        json.dump(state, f, indent=4)

# --- [í•µì‹¬ 2] ë‚ ì§œ ì²˜ë¦¬ í—¬í¼ ---
def parse_date(date_str):
    """RSS ë‚ ì§œ ë¬¸ìì—´ -> datetime ê°ì²´ ë³€í™˜"""
    try:
        return dateutil.parser.parse(date_str)
    except:
        return datetime.now()

def get_safe_date(entry):
    """
    entryì—ì„œ ë‚ ì§œë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    publishedê°€ ì—†ìœ¼ë©´ updated, date, dc_date ìˆœì„œë¡œ ì°¾ìŠµë‹ˆë‹¤.
    """
    return (entry.get('published') or 
            entry.get('updated') or 
            entry.get('date') or 
            entry.get('dc_date') or 
            "1970-01-01T00:00:00+00:00")

def scrape_article_content(url):
    try:
        time.sleep(random.uniform(0.5, 1.5))
        response = requests.get(url, headers=HEADERS, timeout=5)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸° (ID ìš°ì„ , Class ì°¨ì„ )
        article_body = soup.find(id="articleBody") or soup.find(attrs={"class": "art_body"})
        
        if article_body:
            paragraphs = article_body.find_all('p')
            content = ' '.join([p.get_text().strip() for p in paragraphs])
            return content if content else ""
        return ""
    except Exception as e:
        print(f"   âš ï¸ ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return ""

def fetch_enrich_send(producer):
    # 1. ìƒíƒœ ë¡œë“œ (ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì‹œê°„)
    state = load_last_published_at()
    
    for category, rss_url in RSS_URLS.items():
        print(f"\nğŸ“¡ [{category}] RSS í™•ì¸ ì¤‘...")
        
        feed = feedparser.parse(rss_url)
        
        if not feed.entries:
            continue

        # 2. ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì‹œê°„ í™•ì¸
        last_seen_str = state.get(category, "2000-01-01T00:00:00+00:00")
        last_seen_dt = parse_date(last_seen_str)
        
        new_entries = []
        
        # 3. ìƒˆë¡œìš´ ë‰´ìŠ¤ í•„í„°ë§
        for entry in feed.entries:
            # [ìˆ˜ì •] í—¬í¼ í•¨ìˆ˜ë¥¼ í†µí•´ ì•ˆì „í•˜ê²Œ ë‚ ì§œ ë¬¸ìì—´ íšë“
            entry_date_str = get_safe_date(entry)
            entry_dt = parse_date(entry_date_str)
            
            # ì´ë¯¸ ë³¸ ë‰´ìŠ¤(ê³¼ê±° ì‹œê°„)ë¼ë©´ ì¤‘ë‹¨
            if entry_dt <= last_seen_dt:
                break
                
            new_entries.append(entry)

        if not new_entries:
            print(f"   âœ… ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. (Last: {last_seen_str})")
            continue

        print(f"   ğŸš€ {len(new_entries)}ê°œì˜ **ìƒˆë¡œìš´** ê¸°ì‚¬ ë°œê²¬! ìˆ˜ì§‘ ì‹œì‘...")
        
        # ì´ë²ˆ ë°°ì¹˜ì˜ ê°€ì¥ ìµœì‹  ë‚ ì§œ ì €ì¥ (ë‹¤ìŒë²ˆ ë¹„êµ ê¸°ì¤€ì´ ë¨)
        latest_date_in_batch = get_safe_date(new_entries[0])

        for entry in new_entries:
            # [ìˆ˜ì •] ì•ˆì „í•œ ë‚ ì§œ ì‚¬ìš©
            date_str = get_safe_date(entry)
            
            # [ì—¬ê¸°ê°€ ìˆ˜ì •ë¨!] entry.published ëŒ€ì‹  date_str ì‚¬ìš©
            print(f"   Processing: {entry.title[:20]}... ({date_str})")
            
            full_content = scrape_article_content(entry.link)
            
            news_data = {
                "category": category,
                "published_at": date_str, # ì•ˆì „í•œ ë‚ ì§œ ì €ì¥
                "title": entry.title,
                "link": entry.link,
                "summary": getattr(entry, 'summary', ''),
                "content": full_content,
                "feed_last_build_date": latest_date_in_batch
            }
            
            # Kafka ì „ì†¡
            producer.send(KAFKA_TOPIC, value=news_data)
            
            # CSV ì €ì¥
            with open(CSV_FILE, 'a', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=news_data.keys())
                if f.tell() == 0: writer.writeheader()
                writer.writerow(news_data)

        # 4. ìƒíƒœ ì—…ë°ì´íŠ¸
        state[category] = latest_date_in_batch
        save_last_published_at(state)
        producer.flush()

    print("\nâœ… ì‚¬ì´í´ ì™„ë£Œ.")

if __name__ == "__main__":
    producer = create_producer()
    if producer:
        fetch_enrich_send(producer)
        producer.close()